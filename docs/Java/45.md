---
layout: default
title: Ollama 는 mac m1 환경에서 docker 실행하기엔 너무 느렸다...
date: 2025-11-17
parent: 📌 Server
---

m1 에서 docker 로 ollama gemma3 모델 서빙할려고 했는데 너무 느림... 그래서 찾아보니 ollama 측에 이슈 봤는데 여기서 답변을 잘 해줌.

https://github.com/ollama/ollama/issues/3849

맥에서는 docker 에 **gpu 메탈가속**을 공유하지 못해서 느리다고 함. 그래서 그냥 맥에서 직접 서빙해야한다. 여기서 모델 하나만 쓴다고 가정한다면 ollama 에서 모델에 리소스를 집중시킬 수 있고 몇 가지 파라미터로 병렬성을 조절할 수 있다. 이걸 시스템 변수로 설정해주고 해당 프로세스에서 export 해주면 된다.

```bash
ghkdqhrbals@ghkdqhrbalsui-MacBookPro client % export OLLAMA_NUM_PARALLEL=16
ghkdqhrbals@ghkdqhrbalsui-MacBookPro client % export OLLAMA_MAX_LOADED_MODELS=1 
```

이 때 큐를 너무 크게 잡지않는게 좋음. 언제까지 기다릴 수 없기때문에 빠르게 리소스 부족을 ollama 가 내 서버에게 말해주는게 좋다.

![](../2025-11-17-15-48-47.png)

긴 컨텍스트는 아니지만 M1 max 64GB ram 에서 gemma3 모델을 ollama 로 서빙할 때 저정도 속도가 나옴. gpu 사용률은 거의 95% 까지 올라가는것을 확인했다(병렬로 풀로드 잘 돌아가고 있음). 당연히 mac -> docker gpu 싱크 지원을 안하기 떄문에 docker 로 돌리는것보다 훨씬 빨랐다.
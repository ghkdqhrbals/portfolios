---
layout: default
title: Ollama ëŠ” mac m1 í™˜ê²½ì—ì„œ docker ì‹¤í–‰í•˜ê¸°ì—” ë„ˆë¬´ ëŠë ¸ë‹¤...
date: 2025-11-17
parent: ğŸ“Œ Server
---

m1 ì—ì„œ docker ë¡œ ollama gemma3 ëª¨ë¸ ì„œë¹™í• ë ¤ê³  í–ˆëŠ”ë° ë„ˆë¬´ ëŠë¦¼... ê·¸ë˜ì„œ ì°¾ì•„ë³´ë‹ˆ ollama ì¸¡ì— ì´ìŠˆ ë´¤ëŠ”ë° ì—¬ê¸°ì„œ ë‹µë³€ì„ ì˜ í•´ì¤Œ.

https://github.com/ollama/ollama/issues/3849

ë§¥ì—ì„œëŠ” docker ì— **gpu ë©”íƒˆê°€ì†**ì„ ê³µìœ í•˜ì§€ ëª»í•´ì„œ ëŠë¦¬ë‹¤ê³  í•¨. ê·¸ë˜ì„œ ê·¸ëƒ¥ ë§¥ì—ì„œ ì§ì ‘ ì„œë¹™í•´ì•¼í•œë‹¤. ì—¬ê¸°ì„œ ëª¨ë¸ í•˜ë‚˜ë§Œ ì“´ë‹¤ê³  ê°€ì •í•œë‹¤ë©´ ollama ì—ì„œ ëª¨ë¸ì— ë¦¬ì†ŒìŠ¤ë¥¼ ì§‘ì¤‘ì‹œí‚¬ ìˆ˜ ìˆê³  ëª‡ ê°€ì§€ íŒŒë¼ë¯¸í„°ë¡œ ë³‘ë ¬ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. ì´ê±¸ ì‹œìŠ¤í…œ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì£¼ê³  í•´ë‹¹ í”„ë¡œì„¸ìŠ¤ì—ì„œ export í•´ì£¼ë©´ ëœë‹¤.

```bash
ghkdqhrbals@ghkdqhrbalsui-MacBookPro client % export OLLAMA_NUM_PARALLEL=16
ghkdqhrbals@ghkdqhrbalsui-MacBookPro client % export OLLAMA_MAX_LOADED_MODELS=1 
```

ì´ ë•Œ íë¥¼ ë„ˆë¬´ í¬ê²Œ ì¡ì§€ì•ŠëŠ”ê²Œ ì¢‹ìŒ. ì–¸ì œê¹Œì§€ ê¸°ë‹¤ë¦´ ìˆ˜ ì—†ê¸°ë•Œë¬¸ì— ë¹ ë¥´ê²Œ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±ì„ ollama ê°€ ë‚´ ì„œë²„ì—ê²Œ ë§í•´ì£¼ëŠ”ê²Œ ì¢‹ë‹¤.

![](../2025-11-17-15-48-47.png)

ê¸´ ì»¨í…ìŠ¤íŠ¸ëŠ” ì•„ë‹ˆì§€ë§Œ M1 max 64GB ram ì—ì„œ gemma3 ëª¨ë¸ì„ ollama ë¡œ ì„œë¹™í•  ë•Œ ì €ì •ë„ ì†ë„ê°€ ë‚˜ì˜´. gpu ì‚¬ìš©ë¥ ì€ ê±°ì˜ 95% ê¹Œì§€ ì˜¬ë¼ê°€ëŠ”ê²ƒì„ í™•ì¸í–ˆë‹¤(ë³‘ë ¬ë¡œ í’€ë¡œë“œ ì˜ ëŒì•„ê°€ê³  ìˆìŒ). ë‹¹ì—°íˆ mac -> docker gpu ì‹±í¬ ì§€ì›ì„ ì•ˆí•˜ê¸° ë–„ë¬¸ì— docker ë¡œ ëŒë¦¬ëŠ”ê²ƒë³´ë‹¤ í›¨ì”¬ ë¹¨ëë‹¤.

# ollama ì„œë²„ ë³‘ë ¬ì§€ì› ì‹¤í–‰ ë° docker ë¬¶ì–´ì„œ ë¡œì»¬ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸


```bash
#!/bin/bash

set -e

echo "=========================================="
echo "Starting Docker containers using docker-compose..."
echo "=========================================="
docker compose -f docker-compose.yml up -d

echo "=========================================="
echo "Starting Ollama server with parallel processing enabled..."
echo "OLLAMA_NUM_PARALLEL: ${OLLAMA_NUM_PARALLEL:-default}"
echo "OLLAMA_MAX_LOADED_MODELS: ${OLLAMA_MAX_LOADED_MODELS:-default}"
echo "OLLAMA_MAX_QUEUE: ${OLLAMA_MAX_QUEUE:-default}"
echo "OLLAMA_FLASH_ATTENTION: ${OLLAMA_FLASH_ATTENTION:-default}"
echo "OLLAMA_KEEP_ALIVE: ${OLLAMA_KEEP_ALIVE:-default}"
echo "=========================================="

ollama serve &

# Ollama ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
echo "Waiting for Ollama server to be ready..."
for i in {1..30}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "Ollama server is ready!"
        break
    fi
    echo "Waiting... ($i/30)"
    sleep 2
done

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸° ì´ê±° gemma3 ë¯¸ë¦¬ ì„¤ì¹˜í•´ë†“ìœ¼ë©´ ì¢‹ìŒ
MODEL_NAME=${OLLAMA_MODEL:-gemma3}

echo "Checking for model: $MODEL_NAME"

# ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
if ollama list | grep -q "$MODEL_NAME"; then
    echo "Model $MODEL_NAME already exists"
else
    echo "Pulling model: $MODEL_NAME (this may take a few minutes)..."
    ollama pull "$MODEL_NAME"
    echo "Model $MODEL_NAME pulled successfully"
fi

# ëª¨ë¸ ëª©ë¡ ì¶œë ¥
echo "Available models:"
ollama list

echo "Ollama is ready with model: $MODEL_NAME"
echo "Server running at http://localhost:11434"

# ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ìœ ì§€
wait
```
---
layout: default
title: 9. ë‹¨ë°©í–¥ DB Sync with Kafka connector
parent: ğŸ“Œ ì‹¤ì‹œê°„ ì±„íŒ…ì„œë²„ í”„ë¡œì íŠ¸
date: 2023-01-04
nav_order: 9
---


created at 2023-01-04
{: .label .label-yellow }


# INDEX
1. [DB sync ì‹œ ê³ ë ¤í• ì ](#1-db-sync-ì‹œ-ê³ ë ¤í• ì )
2. [ì–´ë–»ê²Œ ë‘ ê°œì˜ master DBë¥¼ sync í•´ì•¼í• ê¹Œ?](#2-ì–´ë–»ê²Œ-ë‘-ê°œì˜-master-dbë¥¼-sync-í•´ì•¼í• ê¹Œ)
   1. [Kafka connectorë€?](#2-1-kafka-connectorë€)
   2. [Kafka **Source** connector setting](#2-2-kafka-source-connector-setting)
      1. postgres wal_level ì„¤ì •
      2. debezium connector ì„¤ì •(source)
      3. connector kafka ë“±ë¡(schema configuration)
      4. DBì— ê°’ ì¶”ê°€í–ˆì„ ë•Œ ì‹¤ì œë¡œ Kafkaë¡œ í˜ëŸ¬ê°€ëŠ”ì§€ í™•ì¸
   3. [Kafka **Sink** connector setting](#2-3-kafka-sink-connector-setting)
      1. postgres-kafka-source-connector ì»¨í…Œì´ë„ˆ ì‹¤í–‰
      2. jdbc-connector ì„¤ì¹˜ ë° ì‚½ì…
      3. sink-connector configuration
   4. [**uni-directional DB sink** ê²°ê³¼](#2-4-uni-directional-db-sink-ê²°ê³¼)

í˜„ì¬ ë‹¨ë°©í–¥ê¹Œì§€ êµ¬í˜„ëœ ë²„ì „ì€ v3.1.0ì´ë©°, í•´ë‹¹ ì½”ë“œì˜ ë‹¤ì–‘í•œ ë²„ì „ì€ ì•„ë˜ì˜ ê¹ƒí—ˆë¸Œì— ì¡´ì¬í•œë‹¤.
* [https://github.com/ghkdqhrbals/spring-chatting-server](https://github.com/ghkdqhrbals/spring-chatting-server)

ì•ì„œ ì—°ë™ì— ì¶”ê°€í•  ë¶€ë¶„ì´ ìˆë‹¤. ì±„íŒ…ì„œë¹„ìŠ¤ë¥¼ ë‘ê°œë¡œ ì‹¤í–‰í•˜ëŠ”ë°, ë¬¸ì œëŠ” DBê°€ ì„œë¡œ ë…ë¦½ì´ë¼ëŠ” ì ì´ë‹¤. ë”°ë¼ì„œ ë‘ ê°œì¤‘, ì–´ëŠ DBê°€ INSERT/ALTER ë“±ì´ ëœë‹¤ë©´ ë‹¤ë¥¸ DBë„ ê°™ì€ íŠ¸ëœì ì…˜ì„ ìˆ˜í–‰í•´ì•¼í•œë‹¤. ì¦‰, ë¶„ì‚° DBì´ë©´ì„œ ì„œë¡œ syncë˜ë„ë¡ í•´ì•¼ëœë‹¤.
> ì´ë ‡ê²Œ DBë¥¼ ë”°ë¡œ ë—€ ì´ìœ ëŠ” ìˆ˜í‰í™•ì¥ì‹œí‚¤ê¸° ì¢‹ê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŸ¬í•œ ë‘ ê°œì˜ DB ëª¨ë‘ master DBë¡œ ìˆ˜í–‰ëœë‹¤.

# 1. DB sync ì‹œ ê³ ë ¤í• ì 
ì´ ë¶€ë¶„ì—ì„œ ê³ ë ¤í•  ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
* ë°±ì—… DBì²˜ëŸ¼ ë‹¨ë°˜í–¥ syncê°€ ì•„ë‹Œ ì–‘ë°©í–¥ sync ë¥¼ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì„œë¡œ ë§ë¬¼ë¦¬ëŠ” **ë¬´í•œë£¨í”„ë¥¼ ì¡°ì‹¬**í•´ì•¼í•œë‹¤.
> ë³´í†µ Source/Target DBë¥¼ ì •í•˜ê³  CDC(Change Data Capture)í›„ Syncë¥¼ í•˜ëŠ”ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì´ë‹¤.
> ì–‘ë°©í–¥ syncëŠ” ì¢€ë” ê¹Œë‹¤ë¡œìš´ê²ƒ ê°™ë‹¤. ì•„ë˜ëŠ” ì–‘ë°©í–¥ì— ìˆì–´ ë°œìƒê°€ëŠ¥í•œ ì´ìŠˆ ë° í•´ê²°ë°©ë²•ì´ë‹¤.
>
> Second, you'll need to make sure to **not propagate the same data change forth and back in an infinite loop**. One way of doing so could for instance be an **SMT(ë‹¨ì¼ ë©”ì‹œì§€ ë³€í™˜) which you apply to both sources and which adds a Kafka Connect header property representing the "origin" of a given change**. In your sink connector, you'd then add that origin as an additional column to your data as you update it. The source connector on that side would then have to be set up (e.g. again using an SMT) to ignore all the changes which originate from replication, as opposed to actual data changes e.g. done by a business application.
>
> Issue from [https://groups.google.com/g/debezium/c/YS22DAgFXSc](https://groups.google.com/g/debezium/c/YS22DAgFXSc)


### 1-1. ì—¬ê¸°ì„œ SMTë€?

> Single Message Transforms (SMTs) is a Kafka API that provides a simple interface for manipulating records as they flow through both the source and sink side of your data pipeline.
>
> reference [https://camel.apache.org/camel-kafka-connector/3.18.x/reference/transformers/index.html](https://camel.apache.org/camel-kafka-connector/3.18.x/reference/transformers/index.html)

ìœ„ì˜ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ë²ˆì—­í•˜ìë©´ SMTsëŠ” `source connector` or `sink connector`ì—ì„œ ë ˆì½”ë“œì˜ ì¹¼ëŸ¼ëª…ì´ë‚˜ value ë“±ì„ ë³€ê²½ì‹œì¼œì„œ ì „ë‹¬í•´ì£¼ëŠ” kafka í¸ì˜ê¸°ëŠ¥ rest api ì´ë‹¤. ì•„ë˜ëŠ” ë‹¤ì–‘í•œ ë³€í™˜ë°©ë²•ì´ë‹¤.

> Some common uses for transforms are:
>
>* [Renaming fields](https://docs.confluent.io/platform/current/connect/transforms/replacefield.html#replacefield)( ì¹¼ëŸ¼ëª… ì¬ì •ì˜ )
>* [Masking values](https://docs.confluent.io/platform/current/connect/transforms/maskfield.html#maskfield)( íŠ¹ì • ì¹¼ëŸ¼ì˜ valueë¥¼ **valid null**ë¡œ ë§Œë“¬ ex) {value} to 0 or "" or false )
>* Routing records to topics based on a value( cloud ì—ì„œëŠ” ì•ˆë¨ )
>* [Converting or inserting timestamps into the record](https://docs.confluent.io/platform/current/connect/transforms/timestampconverter.html#timestampconverter)
>* [Manipulating keys, like setting a key from a fieldâ€™s value](https://docs.confluent.io/platform/current/connect/transforms/valuetokey.html#description)( ì¹´í”„ì¹´ëŠ” keyë¥¼ í†µí•´ ì›í•˜ëŠ” ë©”ì„¸ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤. ì´ ë°©ë²•ì€ íŠ¹ì • ì¹¼ëŸ¼ì˜ valueë¥¼ keyë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°©ë²•ì´ë‹¤ )
>
> reference [https://www.confluent.io/blog/kafka-connect-single-message-transformation-tutorial-with-examples/?_ga=2.130915337.76772118.1672804235-1001218784.1670749352&_gac=1.191662808.1671423652.CjwKCAiAkfucBhBBEiwAFjbkr7Bq_5Npm8yLue-N4DKIv4hpPc44IdpcBYN3ITQzeAAdIkGX2Y5wJRoCBYIQAvD_BwE](https://www.confluent.io/blog/kafka-connect-single-message-transformation-tutorial-with-examples/?_ga=2.130915337.76772118.1672804235-1001218784.1670749352&_gac=1.191662808.1671423652.CjwKCAiAkfucBhBBEiwAFjbkr7Bq_5Npm8yLue-N4DKIv4hpPc44IdpcBYN3ITQzeAAdIkGX2Y5wJRoCBYIQAvD_BwE)

# 2. ì–´ë–»ê²Œ ë‘ ê°œì˜ master DBë¥¼ sync í•´ì•¼í• ê¹Œ?
í•„ìëŠ” Kafka connectorì„ ì´ìš©í•˜ì—¬ ì‹±í¬ë¥¼ ë§ì¶”ë ¤í•œë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” Kafka connectorì— ëŒ€í•œ ì´í•´ê°€ ë°”íƒ•ì´ ë˜ì–´ì•¼í•œë‹¤.

## 2-1. Kafka connectorë€?
Kafka connectorì˜ ê¸°ë³¸ì ì¸ í”Œë¡œìš°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
![img](../../../assets/img/kafka/4.png)
1. RDBì— INSERT/UPDATE/ALTER ë“± ë³€ê²½ë˜ëŠ” íŠ¸ëœì ì…˜ì´ ì‹¤í–‰ë˜ê³  TXlogì— ê¸°ë¡ëœë‹¤
2. Kafka Connectorì€ ì´ë¥¼ ì½ê³ (CDC) Kafkaì˜ Topicì— ì‚½ì…í•œë‹¤

ì´ ë•Œ ì´ connectorì„ ìš°ë¦¬ëŠ” **source conenctor**ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ê·¸ë¦¬ê³  `Kafka ---> DB`ë¥¼ ì—°ê²°ì‹œì¼œì£¼ëŠ” connectorì€ **sink connector**ì´ë¼ê³  ë¶€ë¥¸ë‹¤.

ì¼ë‹¨ ë¨¼ì € Kafka source connectorì„ ì„¤ì •í•´ë³´ê³ , kafdropìœ¼ë¡œ ì‹¤ì œ CDCë˜ëŠ”ì§€ ê´€ì°°í•´ë³´ì

## 2-2. Kafka **Source** connector setting
Kafka soruce connectorì€ ì•„ë˜ì˜ 4ê°€ì§€ ê³¼ì •ì„ ê±°ì³ ì„¤ì • ë° í™•ì¸í•  ìˆ˜ ìˆë‹¤.

1. postgres wal_level ì„¤ì •
2. debezium connector ì„¤ì •
3. connector kafka ë“±ë¡
4. DBì— ê°’ ì¶”ê°€í–ˆì„ ë–„ ì‹¤ì œë¡œ Kafkaë¡œ í˜ëŸ¬ê°€ëŠ”ì§€ í™•ì¸

### 2-2-1. postgres wal_level ì„¤ì •
* ë¨¼ì € wal-levelì´ ë¬´ì—‡ì¼ê¹Œ?

WALì€ Write-Ahead Loggingì˜ ì•½ìë¡œ **íŠ¸ëœì ì…˜ë¡œê·¸ì— ì–´ë–¤ì‹ìœ¼ë¡œ ë³€ê²½ëœ ì‚¬í•­ì„ ì €ì¥í•  ì§€ ì •í•˜ëŠ” ì„¤ì •**ì´ë‹¤.

WALì€ í¬ê²Œ Logicalê³¼ Replicaê°€ ì¡´ì¬í•œë‹¤.

* Logical level : ë ˆì½”ë“œ ê°’ì´ ë³€ê²½ë˜ë©´, ë³€ê²½ëœ ë ˆì½”ë“œ ì „ì²´ê°€ ì €ì¥ëœë‹¤.
> ì˜ˆì‹œ [wal_level = logical](https://www.dbi-services.com/blog/postgresql-when-wal_level-to-logical/)
>
>```
>    lsn    | xid |                                  data
>-----------+-----+------------------------------------------------------------------------
> 0/703F738 | 583 | BEGIN 583
> 0/703F738 | 583 | table public.mytab: INSERT: id[integer]:3 name[character varying]:'t3'
> 0/703F838 | 583 | table public.mytab: INSERT: id[integer]:4 name[character varying]:'t4'
> 0/703F8A8 | 583 | COMMIT 583
> 0/703F8E0 | 584 | BEGIN 584
> 0/703F8E0 | 584 | table public.mytab: DELETE: (no-tuple-data)
> 0/703F948 | 584 | COMMIT 584
>```
>
> statement + row ê¸°ë°˜ìœ¼ë¡œ ì €ì¥ëœë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

* Replica level : ë ˆì½”ë“œ ê°’ì´ ë³€ê²½ë˜ë©´, ë ˆì½”ë“œ ë‚´ ë³€ê²½ëœ ê°’ ë¶€ë¶„ë§Œ íŠ¸ëœì ì…˜ ë¡œê·¸ì— ì €ì¥í•œë‹¤.

ê·¸ë¦¬ê³  ì´ëŸ¬í•œ WAL levelì€ postgresì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ replicaë¡œ ì„¤ì •ë˜ì–´ìˆë‹¤(9.4ë²„ì „ ì´í›„ë¶€í„° logcalì„ ì§€ì›í•œë‹¤). ì´ replica levelì€ debezium kafka connectorì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤. ì¦‰, ë ˆì½”ë“œ ì „ì²´ê°’ì´ ì í˜€ìˆëŠ” íŠ¸ëœì ì…˜ ë¡œê·¸(logical level)ë¥¼ ë³´ê³  CDCí•˜ë„ë¡ ì„¤ì •ë˜ì–´ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ default replica levelì„ logicalë¡œ ì•„ë˜ì™€ê°™ì´ ë°”ê¿”ì¤˜ì•¼í•œë‹¤.

```dockerfile
  chatting-db-1:
    container_name: chatting-db-1
    image: postgres:12-alpine
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_USER=postgres
      - POSTGRES_DB=chat1
    expose:
      - "5433" # Publishes 5433 to other containers but NOT to host machine
    ports:
      - "5433:5433"
    volumes:
      - ./backups:/home/backups
    command: -c wal_level=logical -p 5433 # logicalë¡œ ë³€ê²½
    restart: always

  chatting-db-2:
    container_name: chatting-db-2
    image: postgres:12-alpine
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_USER=postgres
      - POSTGRES_DB=chat2
    expose:
      - "5434" # Publishes 5433 to other containers but NOT to host machine
    ports:
      - "5434:5434"
    volumes:
      - ./backups:/home/backups
    command: -c wal_level=logical -p 5434 # logicalë¡œ ë³€ê²½
    restart: always
```


### 2-2-2. debezium connector ì„¤ì •

ì´ì œ DBì„¤ì •ì€ ëë‚¬ê³ , DBì˜ íŠ¸ëœì ì…˜ ë¡œê·¸ì˜ ë³€ê²½ì‚¬í•­ì„ ê´€ì°°(CDC)í•˜ê³  Kafka í† í”½ì— ì‚½ì…í•´ì£¼ëŠ” connectorì„ ì»¨í…Œì´ë„ˆë¡œ ì•„ë˜ì™€ ê°™ì´ ë„ìš¸ ê²ƒì´ë‹¤.

```dockerfile
  # -------- postgres -> kafka source connector --------
  kafka-source-connector:
    image: debezium/connect:1.9
    container_name: postgres-kafka-source-connector
    ports:
      - 8083:8083
    environment:
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
      BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - zookeeper
      - chatting-db-2
```

### 2-2-3. connector kafka ë“±ë¡

ì´ë ‡ê²Œ DB, Connector-Kafka ì„ ë„ì› ë‹¤ë©´ ì´ì œëŠ” ì„œë¡œ ì—°ê²°í•´ì£¼ì–´ì•¼í•  ì°¨ë¡€ì´ë‹¤. ìš°ë¦¬ëŠ” Kafka connectorê°€ ì§€ì›í•˜ëŠ” restapië¥¼ í†µí•´ ì—°ê²°ì‹œì¼œì¤„ ìˆ˜ ìˆë‹¤.

```
POST http://localhost:8083/connectors
{
    "name": "source-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "plugin.name": "pgoutput",
        "database.hostname": "chatting-db-2",
        "database.port": "5434",
        "database.user": "postgres",
        "database.password": "password",
        "database.dbname" : "chat2",
        "database.server.name": "dbserver5434",
        "transforms": "unwrap,addTopicPrefix",

        # messageì˜ schemaë¥¼ after í•„ë“œë¡œë§Œ ì„¤ì •
        # ì´ ë¶€ë¶„ì„ ì„¤ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ sourceì™€ sink connectorì˜ schemaê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤.
        # ì¦‰, kafkaë¡œ ë©”ì„¸ì§€ê°€ í˜ëŸ¬ê°ˆ ìˆœ ìˆì§€ë§Œ kafkaì—ì„œ dbë¡œ sinkê°€ ì§„í–‰ë˜ì§€ ì•Šì„ê²ƒì´ë‹¤.
        "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
        "transforms.addTopicPrefix.type":"org.apache.kafka.connect.transforms.RegexRouter",
        "transforms.addTopicPrefix.regex":"(.*)",
        "transforms.addTopicPrefix.replacement":"$1"
    }
}
```

### 2-2-4. DBì— ê°’ ì¶”ê°€í–ˆì„ ë•Œ ì‹¤ì œë¡œ Kafkaë¡œ í˜ëŸ¬ê°€ëŠ”ì§€ í™•ì¸

ì´ì œëŠ” ì‹¤ì œë¡œ í™•ì¸í•´ ë³¼ ì°¨ë¡€ì´ë‹¤. ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í™•ì¸í•´ë³¼ê²ƒì´ë‹¤.
This is final process.


1. POST to server

    ```
    POST http://localhost:8080/chat/user
    {
      "userId":"aa",
      "userName":"í™©ë³´ê·œë¯¼"
    }
    ```

2. In docker container log

    ```
    chatting-server-2         | 2023-01-05 05:55:07.829  INFO 1 --- [ad | producer-1] chatting.chat.web.ChatController         : ë©”ì„¸ì§€ ì „ì†¡ ì„±ê³µ topic=log-user-add, offset=0, partition=2
    ```

3. See kafka with Kafdrop

    ![img](../../../assets/img/kafka/5.png)

    ![img](../../../assets/img/kafka/6.png)

ì•„ë˜ëŠ” ë°œìƒí•œ ì—ëŸ¬ì™€ í•´ê²°í•œ ë°©ë²•ì— ëŒ€í•´ ì •ë¦¬í–ˆë‹¤.

* ì´ìŠˆ

  ```
  Connector configuration is invalid and contains the following 1 error(s) Error while validating connector config: Connection to localhost:5434 refused
  ```

* ì´ìŠˆ í•´ê²°ë°©ë²• ì •ë¦¬ : [https://github.com/ghkdqhrbals/spring-chatting-server/issues/1](https://github.com/ghkdqhrbals/spring-chatting-server/issues/1)

## 2-3. Kafka **Sink** connector setting
ì! ì´ì œ DB->KafkaëŠ” ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ, Kafka->DBë¡œ Sink connectorì„ êµ¬ì¶•í•´ì•¼í•œë‹¤. ë‹¤ìŒì˜ ë™ì˜ìƒì„ ì°¸ê³ í•˜ì.

[https://youtu.be/2bPx3hfKX04](https://youtu.be/2bPx3hfKX04)

ìœ„ì˜ ë™ì˜ìƒì€ confluentì˜ cloudë¡œ connectorì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤. í•˜ì§€ë§Œ í•„ìëŠ” êµ³ì´ cloudë¡œ ì„¤ì •í•  í•„ìš”ì—†ë‹¤ê³  ìƒê°í–ˆë‹¤. ë”°ë¼ì„œ source connectorì—ì„œ ì‚¬ìš©í•˜ë˜ debezium connector ì»¨í…Œì´ë„ˆì— jdbc-connectorë§Œ ì¶”ê°€í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ê°€ê¸°ë¡œ í•˜ì˜€ë‹¤.

### 2-3-1. postgres-kafka-source-connector ì»¨í…Œì´ë„ˆ ì‹¤í–‰

```dockerfile
  kafka-source-connector:
    image: debezium/connect:1.9
    container_name: postgres-kafka-source-connector
    ports:
      - 8083:8083
    environment:
      CONFIG_STORAGE_TOPIC: __pg.source.config.storage
      OFFSET_STORAGE_TOPIC: __pg.source.offset.storage
      STATUS_STORAGE_TOPIC: __pg.source.status.storage
      PLUGIN_PATH: /kafka/connect # connector í”ŒëŸ¬ê·¸ì¸ ì €ì¥ì†Œ ìœ„ì¹˜
      BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - zookeeper
      - chatting-db-1
      - chatting-db-2
```

### 2-3-2. jdbc-connector ì„¤ì¹˜ ë° ì‚½ì…

ì•„ë˜ì™€ ê°™ì´ shell scriptë¥¼ ì‘ì„±í•´ì„œ ì‹¤í–‰í•œë‹¤.

> í•´ë‹¹ shell scriptë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ë¨¼ì € `postgres-kafka-source-connector` ì»¨í…Œì´ë„ˆê°€ í•„ìš”í•˜ë‹¤.

```shell
#!/bin/bash

echo "(step-1) confluent-hub cli ë‹¤ìš´ë¡œë“œ ë° ì••ì¶•í•´ì œ"
curl -LO http://client.hub.confluent.io/confluent-hub-client-latest.tar.gz
mkdir confluent-etcs | tar -xvzf confluent-hub-client-latest.tar.gz -C confluent-etcs
#
echo "(step-2) confluent-hub í™˜ê²½ë³€ìˆ˜ ì„¤ì •"
echo $(pwd)
export CONFLUENT_HOME=$(pwd)/confluent-etcs
export PATH=$PATH:$CONFLUENT_HOME/bin
#
echo "(step-3) clië¥¼ í†µí•œ jdbc connector ë‹¤ìš´ë¡œë“œ"
$CONFLUENT_HOME/bin/confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.6.0 --component-dir $(pwd)/confluent-etcs
#
echo "(step-4) debezium connector ì»¨í…Œì´ë„ˆì˜ connectorë¦¬ìŠ¤íŠ¸ì— ì‚½ì…"
docker cp $(pwd)/confluent-etcs/confluentinc-kafka-connect-jdbc postgres-kafka-source-connector:/kafka/connect

echo "(step-5) debezium connector ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ìœ¼ë¡œ loading new connector"
docker restart postgres-kafka-source-connector
```

ì´ë ‡ê²Œ ì„¤ì¹˜ë¥¼ ëë‚´ê³  `GET http://localhost:8083/connector-plugins` ì„ ì „ì†¡í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì •ìƒì ìœ¼ë¡œ JdbcSink/SourceConnectorì™€ ì—°ë™ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```
[
    {
        "class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "type": "sink",
        "version": "10.6.0"
    },
    {
        "class": "io.confluent.connect.jdbc.JdbcSourceConnector",
        "type": "source",
        "version": "10.6.0"
    },
    ...
    {
        "class": "io.debezium.connector.postgresql.PostgresConnector",
        "type": "source",
        "version": "1.9.7.Final"
    },
    ...
]
```

### 2-3-3. sink-connector configuration

```
POST http://localhost:8083/connectors
{
    "name": "sink-connector",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "task.max" : 1,
        "topics": "dbserver5434.public.user_table",

        "connection.url": "jdbc:postgresql://chatting-db-1:5433/chat1",
        "connection.user":"postgres",
        "connection.password":"password",

        # table/column ìë™ìƒì„± ë°©ì§€
        # ë‘ê°œì˜ í…Œì´ë¸”ì´ ì´ë¯¸ ë™ì¼í•¨
        "auto.create": "false",
        "auto.evolve": "false",
        "delete.enabled": "true",
        "insert.mode": "upsert",
        "pk.mode": "record_key",
        "tombstones.on.delete": "true",

        # schemaì¼ì¹˜ í™•ì¸ ë° payload ì¶”ì¶œ ê³¼ì •
        "key.converter": "org.apache.kafka.connect.json.JsonConverter",
        "key.converter.schemas.enable": "true",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter.schemas.enable": "true",
        "transforms": "unwrap,addTopicPrefix",
        "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
        "transforms.addTopicPrefix.type":"org.apache.kafka.connect.transforms.RegexRouter",
        "transforms.addTopicPrefix.regex":"(.*)",
        "transforms.addTopicPrefix.replacement":"$1",

        # í•´ë‹¹ í…Œì´ë¸”ì— new row ì‚½ì…
        "table.name.format":"user_table",

        # ëª‡ ê°œì˜ ë©”ì„¸ì§€ë¥¼ ì½ê³  sinkí•  ê²ƒì¸ì§€
        "batch.size": "1"
    }
}
```

ìµœì¢…ì ìœ¼ë¡œ Kafkaì—ì„œ `dbserver5434.public.user_table` í† í”½ì— ëŒ€í•œ schemaëŠ” ì•„ë˜ì™€ ê°™ì´ ì„¤ì •ëœë‹¤.

```
{
   "schema": {
      "type": "struct",
      "fields": [
         {
            "type": "string",
            "optional": false,
            "field": "user_id"
         },
         {
            "type": "string",
            "optional": true,
            "field": "user_name"
         },
         {
            "type": "string",
            "optional": true,
            "field": "user_status"
         }
      ],
      "optional": false,
      "name": "dbserver5434.public.user_table.Value"
   },
   "payload": {
      "user_id": "a",
      "user_name": "a",
      "user_status": "a"
   }
}
```

## 2-4. **uni-directional DB sink** ê²°ê³¼

ì¼ë‹¨ ë‹¨ë°©í–¥ sink ì„¤ì •ì€ ì´ê±¸ë¡œ ëì´ ë‚¬ë‹¤. í•œë²ˆ í™•ì¸í•´ë³´ì.

ë¨¼ì € kafka-connector ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì´í›„ install-jdbc-connector.sh ì‹¤í–‰

```
gyuminhwangbo@Gyuminui-MacBookPro spring-chatting-server % sh ./install-jdbc-connector.sh

(step-1) confluent-hub cli ë‹¤ìš´ë¡œë“œ ë° ì••ì¶•í•´ì œ
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 7584k  100 7584k    0     0  1131k      0  0:00:06  0:00:06 --:--:-- 1587k
mkdir: confluent-etcs: File exists
x share/doc/confluent-hub-client/notices/
x share/doc/confluent-hub-client/licenses/
x share/java/confluent-hub-client/jakarta.ws.rs-api-2.1.6.jar
...

(step-2) confluent-hub í™˜ê²½ë³€ìˆ˜ ì„¤ì •
/Users/gyuminhwangbo/study/spring-chatting-server

(step-3) clië¥¼ í†µí•œ jdbc connector ë‹¤ìš´ë¡œë“œ
Running in a "--no-prompt" mode
Implicit acceptance of the license below:
Confluent Community License
https://www.confluent.io/confluent-community-license
Downloading component Kafka Connect JDBC 10.6.0, provided by Confluent, Inc. from Confluent Hub and installing into /Users/gyuminhwangbo/study/spring-chatting-server/confluent-etcs
Implicit confirmation of the question: Do you want to uninstall existing version 10.6.0?
...

(step-4) debezium connector ì»¨í…Œì´ë„ˆì˜ connectorë¦¬ìŠ¤íŠ¸ì— ì‚½ì…

(step-5) debezium connector ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ìœ¼ë¡œ loading new connector
postgres-kafka-source-connector
```

ê·¸ë¦¬ê³  connector configuration uploadë¥¼ í•˜ì˜€ë‹¤.

ì•„ë˜ëŠ” DB:5434ê³¼ ì—°ê²°ëœ chatServerì˜ api gateway(nginx)ì— user_tableì˜ insert apië¥¼ ë‚ ë ¸ì„ ë•Œ í„°ë¯¸ë„ ìƒí™©ì´ë‹¤.

```
# (1) kafka-connectorê°€ postgresDB:5434ì˜ Txlogì—ì„œ ë³€ê²½ì‚¬í•­ ê°ì§€(CDC)
postgres-kafka-source-connector | 2023-01-08 07:19:07,589 INFO   Postgres|dbserver5434|streaming  First LSN 'LSN{0/168BF58}' received   [io.debezium.connector.postgresql.connection.WalPositionLocator]
postgres-kafka-source-connector | 2023-01-08 07:19:07,589 INFO   Postgres|dbserver5434|streaming  WAL resume position 'LSN{0/168BF58}' discovered   [io.debezium.connector.postgresql.PostgresStreamingChangeEventSource]
postgres-kafka-source-connector | 2023-01-08 07:19:07,593 INFO   Postgres|dbserver5434|streaming  Connection gracefully closed   [io.debezium.jdbc.JdbcConnection]
postgres-kafka-source-connector | 2023-01-08 07:19:07,600 INFO   Postgres|dbserver5434|streaming  Connection gracefully closed   [io.debezium.jdbc.JdbcConnection]

# (Additional) POST ë°˜í™˜ê°’
nginx                     | 192.168.240.1 - - [08/Jan/2023:07:19:07 +0000] "POST /chat/user HTTP/1.1" 200 86 "-" "PostmanRuntime/7.29.2" "-"

# (2) PgOutput - postgresê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” replicaì„¤ì •ì„ ì•ì„œ ìš°ë¦¬ê°€
# logicalë¡œ ë°”ê¿§ì—ˆë‹¤. ê·¸ë¦¬ê³  logicalë¡œ ì €ì¥ëœ TXlogë“¤ì„ ë””ì½”ë”©í•´ì„œ kafkaì— ë°€ì–´ë„£ê¸° ìœ„í•´
# PgOutputì´ë¼ëŠ” ëª¨ë“ˆì„ í†µí•´ logicalTX----(decoding-PgOutput)---->kafkaë¥¼ ìˆ˜í–‰í•œë‹¤.
postgres-kafka-source-connector | 2023-01-08 07:19:07,614 INFO   Postgres|dbserver5434|streaming  Initializing PgOutput logical decoder publication   [io.debezium.connector.postgresql.connection.PostgresReplicationConnection]
chatting-db-2             | 2023-01-08 07:19:07.620 UTC [82] LOG:  starting logical decoding for slot "debezium"
chatting-db-2             | 2023-01-08 07:19:07.620 UTC [82] DETAIL:  Streaming transactions committing after 0/168BDD8, reading WAL from 0/168BDD8.
chatting-db-2             | 2023-01-08 07:19:07.620 UTC [82] STATEMENT:  START_REPLICATION SLOT "debezium" LOGICAL 0/168BDD8 ("proto_version" '1', "publication_names" 'dbz_publication')
chatting-db-2             | 2023-01-08 07:19:07.620 UTC [82] LOG:  logical decoding found consistent point at 0/168BDD8
chatting-db-2             | 2023-01-08 07:19:07.620 UTC [82] DETAIL:  There are no running transactions.
chatting-db-2             | 2023-01-08 07:19:07.620 UTC [82] STATEMENT:  START_REPLICATION SLOT "debezium" LOGICAL 0/168BDD8 ("proto_version" '1', "publication_names" 'dbz_publication')
postgres-kafka-source-connector | 2023-01-08 07:19:07,636 INFO   Postgres|dbserver5434|streaming  Requested thread factory for connector PostgresConnector, id = dbserver5434 named = keep-alive   [io.debezium.util.Threads]
postgres-kafka-source-connector | 2023-01-08 07:19:07,637 INFO   Postgres|dbserver5434|streaming  Creating thread debezium-postgresconnector-dbserver5434-keep-alive   [io.debezium.util.Threads]
postgres-kafka-source-connector | 2023-01-08 07:19:07,638 INFO   Postgres|dbserver5434|streaming  Processing messages   [io.debezium.connector.postgresql.PostgresStreamingChangeEventSource]
postgres-kafka-source-connector | 2023-01-08 07:19:07,654 INFO   Postgres|dbserver5434|streaming  Message with LSN 'LSN{0/168BF58}' arrived, switching off the filtering   [io.debezium.connector.postgresql.connection.WalPositionLocator]
postgres-kafka-source-connector | 2023-01-08 07:19:08,264 INFO   ||  1 records sent during previous 00:01:18.579, last recorded offset: {transaction_id=null, lsn_proc=23641944, lsn=23641944, txId=501, ts_usec=1673162347279067}   [io.debezium.connector.common.BaseSourceTask]

# (Additional) ì´ê±´ ê·¸ëƒ¥ ë³„ê°œë¡œ connector ì•ˆê±°ì¹˜ê³  ë°”ë¡œ kafkaì— ì‚½ì…í•˜ëŠ” ë³„ë„ì˜ pipeline.
chatting-server-2         | 2023-01-08 07:19:07.806  INFO 1 --- [ad | producer-1] chatting.chat.web.ChatController         : ë©”ì„¸ì§€ ì „ì†¡ ì„±ê³µ topic=log-user-add, offset=0, partition=1

# (3) JDBC-Sink connectorê°€ kafka-topicì˜ ì†Œë¹„ëœ messageì˜ last offsetì„ í™•ì¸í•˜ê³ ,
# ì‹ ê·œ ë°ì´í„° ë°œê²¬, postgresqlì— ëŒ€í•œ dialectë¥¼ ë§Œë“¤ì–´ì„œ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰ì‹œí‚¤ëŠ” ê³¼ì •
postgres-kafka-source-connector | 2023-01-08 07:19:08,287 INFO   ||  [Producer clientId=connector-producer-source-connector-0] Resetting the last seen epoch of partition dbserver5434.public.user_table-0 to 0 since the associated topicId changed from null to oCJqYdENQ1C2cPZeNEhtsw   [org.apache.kafka.clients.Metadata]
postgres-kafka-source-connector | 2023-01-08 07:19:08,313 INFO   ||  Attempting to open connection #1 to PostgreSql   [io.confluent.connect.jdbc.util.CachedConnectionProvider]
postgres-kafka-source-connector | 2023-01-08 07:19:08,412 INFO   ||  Maximum table name length for database is 63 bytes   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
postgres-kafka-source-connector | 2023-01-08 07:19:08,412 INFO   ||  JdbcDbWriter Connected   [io.confluent.connect.jdbc.sink.JdbcDbWriter]
postgres-kafka-source-connector | 2023-01-08 07:19:08,430 INFO   ||  Checking PostgreSql dialect for existence of TABLE "user_table"   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
postgres-kafka-source-connector | 2023-01-08 07:19:08,440 INFO   ||  Using PostgreSql dialect TABLE "user_table" present   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
postgres-kafka-source-connector | 2023-01-08 07:19:08,456 INFO   ||  Checking PostgreSql dialect for type of TABLE "user_table"   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
postgres-kafka-source-connector | 2023-01-08 07:19:08,460 INFO   ||  Setting metadata for table "user_table" to Table{name='"user_table"', type=TABLE columns=[Column{'user_name', isPrimaryKey=false, allowsNull=true, sqlType=varchar}, Column{'user_id', isPrimaryKey=true, allowsNull=false, sqlType=varchar}, Column{'user_status', isPrimaryKey=false, allowsNull=true, sqlType=varchar}]}   [io.confluent.connect.jdbc.util.TableDefinitions]
```


ë“œë””ì–´! ë‹¨ë°©í–¥ ì„¤ì •ì´ ëë‚¬ë‹¤. ì´ì œëŠ” ì–‘ë°©í–¥ì´ ë‚¨ì•˜ë‹¤. í¬ìŠ¤íŒ…ì´ ë„ˆë¬´ ê¸¸ì–´ì ¸ì„œ ì–‘ë°©í–¥ ì„¤ê³„ëŠ” ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œ ì„¤ê³„í•˜ê² ë‹¤. ì•„ë˜ëŠ” ê³ ë ¤í•˜ëŠ” ì–‘ë°©í–¥ DB syncì˜ ì•„í‚¤í…ì²˜ì´ë‹¤.

![img](../../../assets/img/kafka/7.jpg)



# Reference
* [https://www.confluent.io/blog/sync-databases-and-remove-silos-with-kafka-cdc/](https://www.confluent.io/blog/sync-databases-and-remove-silos-with-kafka-cdc/)
* [One way DB sync](https://dbconvert.com/blog/what-is-database-synchronization/)
* [Bi-directional DB sync](https://dbconvert.com/blog/bidirectional-database-synchronization/)
* [https://medium.com/event-driven-utopia/configuring-debezium-to-capture-postgresql-changes-with-docker-compose-224742ca5372](https://medium.com/event-driven-utopia/configuring-debezium-to-capture-postgresql-changes-with-docker-compose-224742ca5372)
* [https://stackoverflow.com/questions/59978213/debezium-could-not-access-file-decoderbufs-using-postgres-11-with-default-plug](https://stackoverflow.com/questions/59978213/debezium-could-not-access-file-decoderbufs-using-postgres-11-with-default-plug)
* [source connector configuration ë¬¸ë²• with debezium Postgres connector](https://debezium.io/documentation/reference/stable/connectors/postgresql.html)
* [Debeziumì„ ì´ìš© source/sink connector ì„¤ì •](https://blog.devgenius.io/change-data-capture-from-mysql-to-postgresql-using-kafka-connect-and-debezium-ae8740ef3a1d)

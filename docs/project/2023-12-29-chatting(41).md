---
layout: default
title: 41. 서버성능 개선기록 - 🟢TPS 31%, MTTFB[p99] 39% 개선
parent: 📌 실시간 채팅서버 프로젝트
nav_order: 41
---

created at 2023-12-29
{: .label .label-yellow }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

## 개선 이전(Ingress 파드 1개)
<img width="1412" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/d2c0eff9-8bbc-44db-a24e-4e36a27acf35">

thread 300 으로 요청 전송 시 위와같이 많은 cpu 로드가 걸리네요. 
cpu limit 설정은 적용되어 있지 않아 보이지만 일반적으로 Ingress 파드 하나로 돌리기에는 많은 부하를 혼자 감당하고 있는것을 확인할 수 있었습니다.

## Ingress 파드 replicaSet=2 각 파드 CPU 리소스 소모 관찰

<img width="1001" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/78afa556-691e-421a-9f68-2bf7f1f4f4a0">

<img width="1002" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/3d5041fd-8faa-420e-b507-c1c6b1ada47f">


AWS **NLB** 에서 각 pods 로 로드밸런싱을 수행하게 되고, 그 결과로 CPU Usage 또한 분산될 수 있었습니다!

## Ingress Pod 가 **2**개일 때 성능지표 측정

| Metric             | Ingress Pod 1 | Ingress Pod 2 | Change            |
|--------------------|---------------|---------------|-------------------|
| Total Tests        | 181,050       | 240,587       | 32.93% 🟢         |
| Error Rate         | 0.00%(0)      | 0.00%(3)      | N/A               |
| TPS 평균 (Average)  | 312.16        | 410.55        | 31.51% 🟢         |
| TPS p50            | 319.00        | 422.50        | 32.38% 🟢         |
| TPS p95            | 217.45        | 288.60        | 32.69% 🟢         |
| TPS p99            | 132.28        | 147.62        | 11.62% 🟢         |
| TPS p99.9          | 96.52         | 37.04         | -61.68% 🔴        |
| MTTFB 평균 (Average)| 950.89 ms     | 709.86 ms     | -25.29% 🟢        |
| MTTFB p50          | 919.20 ms     | 693.65 ms     | -24.54% 🟢        |
| MTTFB p95          | 1322.11 ms    | 958.64 ms     | -27.49% 🟢        |
| MTTFB p99          | 1833.22 ms    | 1117.45 ms    | -39.05% 🟢        |
| MTTFB p99.9        | 2099.12 ms    | 1396.80 ms    | -33.54% 🟢        |
| MTTFB 차이 평균 (Average Difference)| 112.52 ms     | 58.82 ms      | -47.66% 🟢        |
| MTTFB 평균적인 변동률 (Average Variability)| 10.67%        | 7.67%         | -28.09% 🟢        |


정말 큰 폭으로 TPS 와 MTTFB 지표가 변한것을 확인할 수 있습니다. TPS p99.9 는 낮아졌지만, 일반적인 경우에는 모두 지표들이 상승했습니다. 서버의 안전성이 크게 향상되었죠!
파드가 2개로 늘어났을 때 이정도라면 단순히 생각했을 때 3개의 Ingress Pod 라면 어떨까요?

## Ingress Pod 가 **3**개일 때 성능지표 측정

| Metric             | Ingress Pod 2 | Ingress Pod 3 | Change     |
|--------------------|---------------|---------------|------------|
| Total Tests        | 240,587       | 259,702       | 7.94% 🟢   |
| Error Rate         | 0.00%(3)      | 0.08%(206)    | 0.1% 이내    |
| TPS 평균 (Average)  | 410.55        | 442.83        | 7.86% 🟢   |
| TPS p50            | 422.50        | 458.00        | 8.38% 🟢   |
| TPS p95            | 288.60        | 250.80        | -13.08% 🔴 |
| TPS p99            | 147.62        | 119.48        | -19.01% 🔴 |
| TPS p99.9          | 37.04         | 59.51         | 60.69% 🟢  |
| MTTFB 평균 (Average)| 709.86 ms     | 651.05 ms     | -8.30% 🟢  |
| MTTFB p50          | 693.65 ms     | 631.72 ms     | -8.92% 🟢  |
| MTTFB p95          | 958.64 ms     | 1017.44 ms    | 6.12% 🔴   |
| MTTFB p99          | 1117.45 ms    | 1265.16 ms    | 13.22% 🔴  |
| MTTFB p99.9        | 1396.80 ms    | 1407.03 ms    | 0.73% 🔴   |
| MTTFB 차이 평균 (Average Difference)| 58.82 ms      | 88.51 ms      | 50.27% 🔴  |
| MTTFB 평균적인 변동률 (Average Variability)| 7.67%         | 11.53%        | 49.01% 🔴  |


TPS, MTTFB 평균값이 소폭 상승한 것을 확인할 수 있었습니다. **하지만 최악의 시나리오를 가정할 수 있는 p95, p99 측정은 MTTFB 가 조금 낮아진 것을 확인할 수 있었습니다.** 조금 이상하죠? 보통 TPS 와 MTTFB 는 비례관계라고 생각하는데 위의 결과는 반대인 부분이 이상합니다. 이유가 뭘까요? 차근차근 분석해볼게요. 본 분석은 17:05분, 17:45분 시작된 10분동안의 서버 로드테스트의 각 지표를 확인할 것입니다.

### Ingress Pod 늘려도 왜 성능증가안하나요?

1개에서 2개로 갈 땐 꽤 잘 증가했지만, 3개로 스케일 아웃했을 땐 조금 애매하죠. 이유를 찾아보겠습니다.

1. HTTP Request Response 확인

<img width="1410" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/73185a6e-e11e-40cc-983e-aa949ce6ee47">

Request 수의 차이는 없군요! 하지만 응답시간이 갑자기 확 증가한 부분이 보이죠? 이 시점에서의 다른 지표들을 관찰해볼게요.

2. 메모리 GC 에 소요된 시간 및 GC count

여기서 우리는 minor GC 의 빈도가 확 높아졌고 latency 또한 증가하는 것을 확인할 수 있습니다. 뿐만 아니라 majorGC 도 발생했습니다. 왜이렇게 증가했을까요? ㅜㅜ

<img width="1412" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/7a6d18c8-be9d-4147-ae4b-ff2ac00bcf93">

3. Heap GC 로 인한 Stop The World 지연시간 확인

minor GC 가 테스트 동안 50번 정도 발생했으며, 지연시간은 약 8ms 로 아직 과하지 않은 힙 사용량을 확인할 수 있었습니다.

<img width="1416" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/943c1214-3d93-4415-9908-02e40a46ebc8">

결국 왜 이런 결과가 나온지에 대한 분석은 하지 못했습니다... 하지만 **현재 서버에서 동시유저 300명을 핸들링하기 위해 필요한 Ingress Pod 는 2개**라는 것을 알았습니다. 최적화 부분에서 최적의 Ingress Pod 개수를 안 것이죠!